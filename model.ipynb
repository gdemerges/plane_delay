{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (0,1,3,4,10,11,13,19,20,21,22,30,36,41,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n",
      "/var/folders/rt/0hb6rtq14291d__lx9m1jbd00000gn/T/ipykernel_25602/1115661823.py:9: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fichier, on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TAXI_IN DEST_AIRPORT_ID  CARRIER_DELAY DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0            5.0           12478            NaN    0600-0659             1.0   \n",
      "1            7.0           12478            NaN    0600-0659             1.0   \n",
      "2            8.0           12478            NaN    0600-0659             1.0   \n",
      "3            6.0           12478            NaN    0600-0659             1.0   \n",
      "4            5.0           12478            NaN    0600-0659             1.0   \n",
      "...          ...             ...            ...          ...             ...   \n",
      "5635973     10.0           12892            NaN    0600-0659             2.0   \n",
      "5635974      4.0           13232            NaN    0700-0759             6.0   \n",
      "5635975      3.0           14679            NaN    2000-2059             2.0   \n",
      "5635976      2.0           14679            NaN    1400-1459             2.0   \n",
      "5635977      5.0           14679            NaN    1000-1059             2.0   \n",
      "\n",
      "            FL_DATE ORIGIN  ARR_DELAY_NEW ORIGIN_STATE_NM  ARR_DELAY  ...  \\\n",
      "0        2016-09-01    BOS            0.0   Massachusetts      -23.0  ...   \n",
      "1        2016-09-02    BOS            0.0   Massachusetts       -6.0  ...   \n",
      "2        2016-09-06    BOS            0.0   Massachusetts      -19.0  ...   \n",
      "3        2016-09-08    BOS            0.0   Massachusetts      -27.0  ...   \n",
      "4        2016-09-09    BOS            0.0   Massachusetts      -21.0  ...   \n",
      "...             ...    ...            ...             ...        ...  ...   \n",
      "5635973  2016-02-29    TUS            0.0         Arizona      -11.0  ...   \n",
      "5635974  2016-02-29    TUS            0.0         Arizona       -6.0  ...   \n",
      "5635975  2016-02-29    TUS            0.0         Arizona      -17.0  ...   \n",
      "5635976  2016-02-29    TUS            0.0         Arizona       -9.0  ...   \n",
      "5635977  2016-02-29    TUS            0.0         Arizona       -9.0  ...   \n",
      "\n",
      "         ORIGIN_CITY_MARKET_ID ARR_DELAY_GROUP  CRS_ELAPSED_TIME MONTH  \\\n",
      "0                        30721            -2.0              75.0     9   \n",
      "1                        30721            -1.0              75.0     9   \n",
      "2                        30721            -2.0              75.0     9   \n",
      "3                        30721            -2.0              75.0     9   \n",
      "4                        30721            -2.0              75.0     9   \n",
      "...                        ...             ...               ...   ...   \n",
      "5635973                  30436            -1.0              95.0     2   \n",
      "5635974                  30436            -1.0             200.0     2   \n",
      "5635975                  30436            -2.0              75.0     2   \n",
      "5635976                  30436            -1.0              75.0     2   \n",
      "5635977                  30436            -1.0              75.0     2   \n",
      "\n",
      "         AIRLINE_ID QUARTER  CANCELLED  DEST_CITY_MARKET_ID  \\\n",
      "0             19805       3        0.0                31703   \n",
      "1             19805       3        0.0                31703   \n",
      "2             19805       3        0.0                31703   \n",
      "3             19805       3        0.0                31703   \n",
      "4             19805       3        0.0                31703   \n",
      "...             ...     ...        ...                  ...   \n",
      "5635973       19393       1        0.0                32575   \n",
      "5635974       19393       1        0.0                30977   \n",
      "5635975       19393       1        0.0                33570   \n",
      "5635976       19393       1        0.0                33570   \n",
      "5635977       19393       1        0.0                33570   \n",
      "\n",
      "         ORIGIN_STATE_FIPS DEST_STATE_FIPS  \n",
      "0                     25.0            36.0  \n",
      "1                     25.0            36.0  \n",
      "2                     25.0            36.0  \n",
      "3                     25.0            36.0  \n",
      "4                     25.0            36.0  \n",
      "...                    ...             ...  \n",
      "5635973                4.0             6.0  \n",
      "5635974                4.0            17.0  \n",
      "5635975                4.0             6.0  \n",
      "5635976                4.0             6.0  \n",
      "5635977                4.0             6.0  \n",
      "\n",
      "[5635978 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "dossier_csv = 'data/'\n",
    "\n",
    "fichiers_csv = glob.glob(dossier_csv + '*.csv')\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for fichier in fichiers_csv:\n",
    "    try:\n",
    "        df = pd.read_csv(fichier, on_bad_lines='skip')\n",
    "        dataframes.append(df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {fichier}: {e}\")\n",
    "\n",
    "colonnes = set()\n",
    "for df in dataframes:\n",
    "    colonnes.update(df.columns)\n",
    "\n",
    "dataframes = [df.reindex(columns=colonnes) for df in dataframes]\n",
    "\n",
    "df_concatene = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(df_concatene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles après suppression :\n",
      "Index(['ARR_DELAY_NEW', 'ARR_DELAY', 'YEAR', 'DAY_OF_MONTH', 'UNIQUE_CARRIER',\n",
      "       'ORIGIN_CITY_NAME', 'DEST_CITY_NAME', 'MONTH'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "colonnes_a_retirer = [\n",
    "    'QUARTER', 'FL_DATE', 'AIRLINE_ID', 'CARRIER', 'ORIGIN_AIRPORT_ID', 'CRS_DEP_HOUR', 'ORIGIN_AIRPORT_SEQ_ID',\n",
    "    'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS',\n",
    "    'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID',\n",
    "    'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'DEP_DELAY_NEW',\n",
    "    'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'WHEELS_OFF', 'WHEELS_ON',\n",
    "    'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'FLIGHTS', 'FIRST_DEP_TIME', 'TOTAL_ADD_GTIME',\n",
    "    'LONGEST_ADD_GTIME', 'DISTANCE_GROUP', 'TAIL_NUM', 'WEATHER_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
    "    'SECURITY_DELAY', 'TAXI_IN', 'FL_NUM', 'DEST', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'AIR_TIME',\n",
    "    'DEP_TIME', 'DISTANCE', 'ACTUAL_ELAPSED_TIME', 'DIVERTED', 'CANCELLATION_CODE', 'DEP_DELAY', 'ARR_TIME',\n",
    "    'NAS_DELAY', 'TAXI_OUT', 'ORIGIN', 'CANCELLED', 'Unnamed: 64', 'CARRIER_DELAY', 'CRS_ELAPSED_TIME', \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "df_concatene = df_concatene.drop(columns=colonnes_a_retirer, errors='ignore')\n",
    "\n",
    "print(\"Colonnes disponibles après suppression :\")\n",
    "print(df_concatene.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_percentage = df_concatene.isnull().mean() * 100\n",
    "high_missing_cols = missing_values_percentage[missing_values_percentage > 50]\n",
    "df_cleaned = df_concatene.drop(columns=high_missing_cols.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column):\n",
    "    df['z_score'] = zscore(df[column])\n",
    "    df_filtered = df[(df['z_score'] > -3) & (df['z_score'] < 3)]\n",
    "    df_filtered = df_filtered.drop(columns=['z_score'])\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = remove_outliers(df_cleaned, 'ARR_DELAY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonnes numériques :\n",
      "['ARR_DELAY_NEW', 'ARR_DELAY', 'MONTH']\n",
      "\n",
      "Colonnes catégorielles :\n",
      "['YEAR', 'DAY_OF_MONTH', 'UNIQUE_CARRIER', 'ORIGIN_CITY_NAME', 'DEST_CITY_NAME']\n",
      "\n",
      "Types des colonnes après conversion :\n",
      "ARR_DELAY_NEW       float64\n",
      "ARR_DELAY           float64\n",
      "YEAR                 object\n",
      "DAY_OF_MONTH         object\n",
      "UNIQUE_CARRIER       object\n",
      "ORIGIN_CITY_NAME     object\n",
      "DEST_CITY_NAME       object\n",
      "MONTH                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = df_cleaned.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "print(\"\\nColonnes numériques :\")\n",
    "print(numeric_columns)\n",
    "\n",
    "print(\"\\nColonnes catégorielles :\")\n",
    "print(categorical_columns)\n",
    "\n",
    "if 'ARR_DELAY' in numeric_columns:\n",
    "    numeric_columns.remove('ARR_DELAY')\n",
    "\n",
    "for col in numeric_columns:\n",
    "    df_cleaned[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_cleaned[col] = df[col].astype(str)\n",
    "\n",
    "print(\"\\nTypes des colonnes après conversion :\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5635978 entries, 0 to 5635977\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   ARR_DELAY_NEW     float64\n",
      " 1   ARR_DELAY         float64\n",
      " 2   YEAR              object \n",
      " 3   DAY_OF_MONTH      object \n",
      " 4   UNIQUE_CARRIER    object \n",
      " 5   ORIGIN_CITY_NAME  object \n",
      " 6   DEST_CITY_NAME    object \n",
      " 7   MONTH             int64  \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 344.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_concatene.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les retards en classes (0: pas de retard, 1: retard)\n",
    "df_cleaned['Delayed'] = (df_cleaned['ARR_DELAY_NEW'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de valeurs dans la colonne 'Delayed' :\n",
      "0    300414\n",
      "1    123475\n",
      "Name: Delayed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "delayed_counts = df_cleaned['Delayed'].value_counts()\n",
    "print(\"\\nNombre de valeurs dans la colonne 'Delayed' :\")\n",
    "print(delayed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7083677369128784\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83     60054\n",
      "           1       0.00      0.00      0.00     24724\n",
      "\n",
      "    accuracy                           0.71     84778\n",
      "   macro avg       0.35      0.50      0.41     84778\n",
      "weighted avg       0.50      0.71      0.59     84778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumedemerges/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/guillaumedemerges/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/guillaumedemerges/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Séparer les caractéristiques et la variable cible\n",
    "X = df_cleaned[['ORIGIN_CITY_NAME', 'DEST_CITY_NAME', 'MONTH', 'DAY_OF_MONTH']]\n",
    "y = df_cleaned['Delayed']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identifier les colonnes numériques et catégorielles\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# Convertir toutes les colonnes catégorielles en chaînes de caractères\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "\n",
    "# Créer les transformateurs pour les colonnes numériques et catégorielles\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Créer le préprocesseur avec ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Créer le pipeline avec le préprocesseur et le classificateur\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# Entraîner le modèle\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identifier les colonnes numériques et catégorielles\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# Convertir toutes les colonnes catégorielles en chaînes de caractères\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "\n",
    "# Créer les transformateurs pour les colonnes numériques et catégorielles\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Créer le préprocesseur avec ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modèle : LogisticRegression\n",
      "Accuracy: 0.5927717096416523\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.59      0.67     60054\n",
      "           1       0.37      0.59      0.46     24724\n",
      "\n",
      "    accuracy                           0.59     84778\n",
      "   macro avg       0.58      0.59      0.57     84778\n",
      "weighted avg       0.66      0.59      0.61     84778\n",
      "\n",
      "Matrice de confusion:\n",
      "[[35557 24497]\n",
      " [10027 14697]]\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(model):\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('undersampler', RandomUnderSampler(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    print(f\"\\nModèle : {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Matrice de confusion:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "train_and_evaluate_model(logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flight_delay_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, 'flight_delay_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destinations.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarder les listes uniques de villes de départ et d'arrivée\n",
    "origins = df['ORIGIN_CITY_NAME'].unique()\n",
    "destinations = df['DEST_CITY_NAME'].unique()\n",
    "\n",
    "joblib.dump(origins, 'origins.pkl')\n",
    "joblib.dump(destinations, 'destinations.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
